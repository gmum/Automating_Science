{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello:\n",
    "\n",
    "Absolutely all credits to the paper Just Train Twice: Improving Group Robustness without Training Group Information \n",
    "\n",
    "and the provided code https://github.com/anniesch/jtt/tree/master\n",
    "\n",
    "the code below was slightly modified and well put together onto one jupyter notebook\n",
    "\n",
    "this code should work with the provided configuration, possibly you have to play around if about to change something. use heavily the original github repo / work directly by cloning jtt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary:\n",
    "\n",
    "- pip install all dependencies listed 2 cells below\n",
    "- download data (0.5GB) https://nlp.stanford.edu/data/dro/waterbird_complete95_forest2water2.tar.gz\n",
    "- unpack and place it in jtt folder\n",
    "- suggested dir structure: \n",
    "\n",
    "{jtt: main.ipynb, requirements.txt, waterbird_complete95_forest2water2, {results: log: {...}}}\n",
    "\n",
    "if you clone Automating_Science repo then add dataset to gitignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file itself, the jupyter notebook is quite lenghty, but some people prefer it that way\n",
    "\n",
    "PS: some features (eg. chi-square geometry or label shift support) is not provided/fails to work in the original repo.\n",
    "\n",
    "**Enjoy** and communicate on Piazza!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for debugging\n",
    "import ipykernel\n",
    "ipykernel.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import csv\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.special import softmax\n",
    "\n",
    "import bisect\n",
    "import warnings\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"resnet50\"\n",
    "model_type = model\n",
    "dataset = \"cub\"\n",
    "\n",
    "join = os.path.join\n",
    "data_path = \"waterbird_complete95_forest2water2\"\n",
    "metadata_path = f\"{data_path}/metadata.csv\"\n",
    "results_dir = f\"results/model_outputs/\"\n",
    "log_dir = f\"results/log/\"\n",
    "root_dir = \"./cub\"\n",
    "\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "fraction = 1.0\n",
    "val_fraction = 0.1\n",
    "\n",
    "output_csv_name = \"aa\"\n",
    "job_script_name = \"bb\"\n",
    "job_script_name = \"cc\"\n",
    "\n",
    "target = \"waterbird_complete95\"\n",
    "confounder_name = [\"forest2water2\"]\n",
    "augment_data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = f\"{dataset}_sample_exp\"\n",
    "train_from_scratch = True\n",
    "resume = False\n",
    "mode = \"w\"\n",
    "if os.path.exists(log_dir) and resume:\n",
    "    resume = True\n",
    "    mode = \"a\"\n",
    "else:\n",
    "    resume = False\n",
    "    mode = \"w\"\n",
    "\n",
    "log_every = 2\n",
    "show_progress = False\n",
    "save_step = 1\n",
    "save_last = False\n",
    "save_best = False\n",
    "\n",
    "n_epochs = 3\n",
    "final_epoch = n_epochs\n",
    "lr = 1e-5\n",
    "max_grad_norm = 1.0 # only bert!\n",
    "batch_size = 64\n",
    "wd = 1.0\n",
    "gamma = 0.1\n",
    "minimum_variational_weight = 0\n",
    "use_bert_params = 1\n",
    "scheduler_flag = False\n",
    "warmup_steps = 0\n",
    "adam_epsilon = 1e-8\n",
    "\n",
    "method = \"ERM\"\n",
    "loss_type = [\"erm\", \"group_dro\", \"joint_dro\"][0]\n",
    "hinge = False\n",
    "btl = False\n",
    "\n",
    "initialization_text = \"\"\"\"\"\"\n",
    "final_text = \"***\"\n",
    "memory = 30\n",
    "seed = 42\n",
    "\n",
    "shift_type = \"confounder\"\n",
    "up_weight = 0\n",
    "fold = None\n",
    "num_folds_per_sweep = 5\n",
    "num_sweeps = 4\n",
    "aug_col =  None\n",
    "reweight_groups = False\n",
    "\n",
    "alpha = 0.2\n",
    "generalization_adjustment = \"0.0\"\n",
    "automatic_adjustment = False\n",
    "robust_step_size = 0.01\n",
    "joint_dro_alpha = 1\n",
    "use_normalized_loss = False\n",
    "\n",
    "conf_threshold = 0.5\n",
    "deploy = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_attributes = {\n",
    "    \"resnet50\": {\n",
    "        \"feature_type\": \"image\",\n",
    "        \"target_resolution\": (224, 224),\n",
    "        \"flatten\": False,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model, pretrained, resume, n_classes, dataset, log_dir, train_data=None):\n",
    "    if train_data is not None:\n",
    "        if resume:\n",
    "            model = torch.load(os.path.join(log_dir, \"last_model.pth\"))\n",
    "            d = train_data.input_size()[0]\n",
    "        elif model_attributes[model][\"feature_type\"] in (\n",
    "                \"precomputed\",\n",
    "                \"raw_flattened\",\n",
    "        ):\n",
    "            assert pretrained\n",
    "            # Load precomputed features\n",
    "            d = train_data.input_size()[0]\n",
    "            model = nn.Linear(d, n_classes)\n",
    "            model.has_aux_logits = False\n",
    "    elif model == \"resnet50\":\n",
    "        model = torchvision.models.resnet50(pretrained=pretrained)\n",
    "        d = model.fc.in_features\n",
    "        model.fc = nn.Linear(d, n_classes)\n",
    "    elif model == \"resnet34\":\n",
    "        model = torchvision.models.resnet34(pretrained=pretrained)\n",
    "        d = model.fc.in_features\n",
    "        model.fc = nn.Linear(d, n_classes)\n",
    "    elif model == \"wideresnet50\":\n",
    "        model = torchvision.models.wide_resnet50_2(pretrained=pretrained)\n",
    "        d = model.fc.in_features\n",
    "        model.fc = nn.Linear(d, n_classes)\n",
    "    else:\n",
    "        raise ValueError(f\"{model} Model not recognized.\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger(object):\n",
    "    def __init__(self, fpath=None, mode=\"w\"):\n",
    "        self.console = sys.stdout\n",
    "        self.file = None\n",
    "        if fpath is not None:\n",
    "            self.file = open(fpath, mode)\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "    def __enter__(self):\n",
    "        pass\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.close()\n",
    "\n",
    "    def write(self, msg):\n",
    "        self.console.write(msg)\n",
    "        if self.file is not None:\n",
    "            self.file.write(msg)\n",
    "\n",
    "    def flush(self):\n",
    "        self.console.flush()\n",
    "        if self.file is not None:\n",
    "            self.file.flush()\n",
    "            os.fsync(self.file.fileno())\n",
    "\n",
    "    def close(self):\n",
    "        self.console.close()\n",
    "        if self.file is not None:\n",
    "            self.file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVBatchLogger:\n",
    "    def __init__(self, csv_path, n_groups, mode=\"w\"):\n",
    "        columns = [\"epoch\", \"batch\"]\n",
    "        for idx in range(n_groups):\n",
    "            columns.append(f\"avg_loss_group:{idx}\")\n",
    "            columns.append(f\"exp_avg_loss_group:{idx}\")\n",
    "            columns.append(f\"avg_acc_group:{idx}\")\n",
    "            columns.append(f\"processed_data_count_group:{idx}\")\n",
    "            columns.append(f\"update_data_count_group:{idx}\")\n",
    "            columns.append(f\"update_batch_count_group:{idx}\")\n",
    "        columns.append(\"avg_actual_loss\")\n",
    "        columns.append(\"avg_per_sample_loss\")\n",
    "        columns.append(\"avg_acc\")\n",
    "        columns.append(\"model_norm_sq\")\n",
    "        columns.append(\"reg_loss\")\n",
    "\n",
    "        self.path = csv_path\n",
    "        self.file = open(csv_path, mode)\n",
    "        self.columns = columns\n",
    "        self.writer = csv.DictWriter(self.file, fieldnames=columns)\n",
    "        if mode == \"w\":\n",
    "            self.writer.writeheader()\n",
    "\n",
    "    def log(self, epoch, batch, stats_dict):\n",
    "        stats_dict[\"epoch\"] = epoch\n",
    "        stats_dict[\"batch\"] = batch\n",
    "        self.writer.writerow(stats_dict)\n",
    "\n",
    "    def flush(self):\n",
    "        self.file.flush()\n",
    "\n",
    "    def close(self):\n",
    "        self.file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRODataset(Dataset):\n",
    "    def __init__(self, dataset, process_item_fn, n_groups, n_classes,\n",
    "                 group_str_fn):\n",
    "        self.dataset = dataset\n",
    "        self.process_item = process_item_fn\n",
    "        self.n_groups = n_groups\n",
    "        self.n_classes = n_classes\n",
    "        self.group_str = group_str_fn\n",
    "        group_array = []\n",
    "        y_array = []\n",
    "\n",
    "        group_array = self.get_group_array()\n",
    "        y_array = self.get_label_array()\n",
    "\n",
    "        self._group_array = torch.LongTensor(group_array)\n",
    "        self._y_array = torch.LongTensor(y_array)\n",
    "        self._group_counts = ((torch.arange(\n",
    "            self.n_groups).unsqueeze(1) == self._group_array).sum(1).float())\n",
    "\n",
    "        self._y_counts = (torch.arange(\n",
    "            self.n_classes).unsqueeze(1) == self._y_array).sum(1).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.process_item is None:\n",
    "            return self.dataset[idx]\n",
    "        else:\n",
    "            return self.process_item(self.dataset[idx])\n",
    "\n",
    "    def get_group_array(self):\n",
    "        if self.process_item is None:\n",
    "            return self.dataset.get_group_array()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def get_label_array(self):\n",
    "        if self.process_item is None:\n",
    "            return self.dataset.get_label_array()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def group_counts(self):\n",
    "        return self._group_counts\n",
    "\n",
    "    def class_counts(self):\n",
    "        return self._y_counts\n",
    "\n",
    "    def input_size(self):\n",
    "        for x, y, g, _ in self:\n",
    "            return x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_data(data, logger):\n",
    "    logger.write(\"Training Data...\\n\")\n",
    "    for group_idx in range(data[\"train_data\"].n_groups):\n",
    "        logger.write(\n",
    "            f'    {data[\"train_data\"].group_str(group_idx)}: n = {data[\"train_data\"].group_counts()[group_idx]:.0f}\\n'\n",
    "        )\n",
    "    logger.write(\"Validation Data...\\n\")\n",
    "    for group_idx in range(data[\"val_data\"].n_groups):\n",
    "        logger.write(\n",
    "            f'    {data[\"val_data\"].group_str(group_idx)}: n = {data[\"val_data\"].group_counts()[group_idx]:.0f}\\n'\n",
    "        )\n",
    "    if data[\"test_data\"] is not None:\n",
    "        logger.write(\"Test Data...\\n\")\n",
    "        for group_idx in range(data[\"test_data\"].n_groups):\n",
    "            logger.write(\n",
    "                f'    {data[\"test_data\"].group_str(group_idx)}: n = {data[\"test_data\"].group_counts()[group_idx]:.0f}\\n'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(dataset, train, reweight_groups, **kwargs):\n",
    "    if not train:  # Validation or testing\n",
    "        assert reweight_groups is None\n",
    "        shuffle = False\n",
    "        sampler = None\n",
    "    elif not reweight_groups:  # Training but not reweighting\n",
    "        shuffle = True\n",
    "        sampler = None\n",
    "    else:  # Training and reweighting\n",
    "        # When the --robust flag is not set, reweighting changes the loss function\n",
    "        # from the normal ERM (average loss over each training example)\n",
    "        # to a reweighted ERM (weighted average where each (y,c) group has equal weight) .\n",
    "        # When the --robust flag is set, reweighting does not change the loss function\n",
    "        # since the minibatch is only used for mean gradient estimation for each group separately\n",
    "        group_weights = len(dataset) / dataset._group_counts\n",
    "        weights = group_weights[dataset._group_array]\n",
    "        # Replacement needs to be set to True, otherwise we'll run out of minority samples\n",
    "        sampler = WeightedRandomSampler(weights,\n",
    "                                        len(dataset),\n",
    "                                        replacement=True)\n",
    "        shuffle = False\n",
    "\n",
    "    # assert shuffle == False\n",
    "    loader = DataLoader(dataset, shuffle=shuffle, sampler=sampler, **kwargs)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Subsets a dataset while preserving original indexing.\n",
    "\n",
    "    NOTE: torch.utils.dataset.Subset loses original indexing.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, indices):\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "\n",
    "        self.group_array = self.get_group_array(re_evaluate=True)\n",
    "        self.label_array = self.get_label_array(re_evaluate=True)\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[self.indices[idx]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def get_group_array(self, re_evaluate=True):\n",
    "        \"\"\"Return an array [g_x1, g_x2, ...]\"\"\"\n",
    "        # setting re_evaluate=False helps us over-write the group array if necessary (2-group DRO)\n",
    "        if re_evaluate:\n",
    "            group_array = self.dataset.get_group_array()[self.indices]        \n",
    "            assert len(group_array) == len(self)\n",
    "            return group_array\n",
    "        else:\n",
    "            return self.group_array\n",
    "\n",
    "    def get_label_array(self, re_evaluate=True):\n",
    "        if re_evaluate:\n",
    "            label_array = self.dataset.get_label_array()[self.indices]\n",
    "            assert len(label_array) == len(self)\n",
    "            return label_array\n",
    "        else:\n",
    "            return self.label_array\n",
    "\n",
    "\n",
    "class ConcatDataset(torch.utils.data.ConcatDataset):\n",
    "    \"\"\"\n",
    "    Concate datasets\n",
    "\n",
    "    Extends the default torch class to support group and label arrays.\n",
    "    \"\"\"\n",
    "    def __init__(self, datasets):\n",
    "        super(ConcatDataset, self).__init__(datasets)\n",
    "\n",
    "    def get_group_array(self):\n",
    "        group_array = []\n",
    "        for dataset in self.datasets:\n",
    "            group_array += list(np.squeeze(dataset.get_group_array()))\n",
    "        return group_array\n",
    "\n",
    "    def get_label_array(self):\n",
    "        label_array = []\n",
    "        for dataset in self.datasets:\n",
    "            label_array += list(np.squeeze(dataset.get_label_array()))\n",
    "        return label_array\n",
    "\n",
    "\n",
    "def get_fold(\n",
    "    dataset,\n",
    "    fold_arg=None,\n",
    "    cross_validation_ratio=0.2,\n",
    "    num_valid_per_point=4,\n",
    "    seed=0,\n",
    "    shuffle=True,\n",
    "):\n",
    "    \"\"\"Returns (train, valid) splits of the dataset.\n",
    "\n",
    "    Args:\n",
    "      dataset (DRODataset): the dataset to split into (train, valid) splits.\n",
    "      cross_validation_ratio (float): valid set size is this times the size of\n",
    "          the dataset.\n",
    "      num_valid_per_point (int): number of times each point appears in a\n",
    "          validation set.\n",
    "      seed (int): under the same seed, the output of this is guaranteed to be\n",
    "          the same.\n",
    "      shuffle (bool): whether to shuffle the training-set for cross validation\n",
    "          or not (used for debugging can be removed later.)\n",
    "\n",
    "    Returns:\n",
    "      folds (list[list[[(DRODataset, DRODataset)]]): the (train, valid) splits.\n",
    "          In each outer list, the inner list valid sets span the entire train\n",
    "          set.  Each inner list is length: num_valid_per_point * 1 /\n",
    "          cross_validation_ratio.\n",
    "    \"\"\"\n",
    "    if fold_arg is not None:\n",
    "        indices = fold_arg.split(\"_\")[1:]\n",
    "        sweep_ind = int(indices[0])\n",
    "        fold_ind = int(indices[1])\n",
    "        assert sweep_ind is None or sweep_ind < num_valid_per_point\n",
    "        assert fold_ind is None or fold_ind < int(1 / cross_validation_ratio)\n",
    "\n",
    "    valid_size = int(np.ceil(len(dataset) * cross_validation_ratio))\n",
    "    num_valid_sets = int(np.ceil(len(dataset) / valid_size))\n",
    "\n",
    "    random = np.random.RandomState(seed)\n",
    "\n",
    "    all_folds = []\n",
    "    for sweep_counter in range(num_valid_per_point):\n",
    "        folds = []\n",
    "        indices = list(range(len(dataset)))\n",
    "        if shuffle:\n",
    "            random.shuffle(indices)\n",
    "        else:\n",
    "            print(\"\\n\" * 10, \"WARNING, NOT SHUFFLING\", \"\\n\" * 10)\n",
    "        for i in range(num_valid_sets):\n",
    "            train_indices = indices[:i * valid_size] + indices[(i + 1) *\n",
    "                                                               valid_size:]\n",
    "            print(\"len(train_indices)\", len(train_indices))\n",
    "            train_split = Subset(dataset, train_indices)\n",
    "\n",
    "            valid_indices = indices[i * valid_size:(i + 1) * valid_size]\n",
    "            print(\"len(valid_indices)\", len(valid_indices))\n",
    "            valid_split = Subset(dataset, valid_indices)\n",
    "            if sweep_counter == 0 and i == 0:\n",
    "                print(\"train_split\", train_split, \"valid_split\", valid_split)\n",
    "            folds.append((train_split, valid_split))\n",
    "        all_folds.append(folds)\n",
    "\n",
    "    if fold_arg is not None:\n",
    "        train_data_subset, val_data_subset = all_folds[sweep_ind][fold_ind]\n",
    "        # Wrap in DRODataset Objects\n",
    "        train_data = DRODataset(\n",
    "            train_data_subset,\n",
    "            process_item_fn=None,\n",
    "            n_groups=dataset.n_groups,\n",
    "            n_classes=dataset.n_classes,\n",
    "            group_str_fn=dataset.group_str,\n",
    "        )\n",
    "\n",
    "        val_data = DRODataset(\n",
    "            val_data_subset,\n",
    "            process_item_fn=None,\n",
    "            n_groups=dataset.n_groups,\n",
    "            n_classes=dataset.n_classes,\n",
    "            group_str_fn=dataset.group_str,\n",
    "        )\n",
    "\n",
    "        return train_data, val_data\n",
    "    else:\n",
    "        return all_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_label_shift(dataset, n_classes, shift_type, minority_frac,\n",
    "                      imbalance_ratio):\n",
    "    assert shift_type.startswith(\"label_shift\")\n",
    "    if shift_type == \"label_shift_step\":\n",
    "        return step_shift(dataset, n_classes, minority_frac, imbalance_ratio)\n",
    "\n",
    "\n",
    "def step_shift(dataset, n_classes, minority_frac, imbalance_ratio):\n",
    "    # get y info\n",
    "    y_array = []\n",
    "    for x, y in dataset:\n",
    "        y_array.append(y)\n",
    "    y_array = torch.LongTensor(y_array)\n",
    "    y_counts = ((\n",
    "        torch.arange(n_classes).unsqueeze(1) == y_array).sum(1)).float()\n",
    "    # figure out sample size for each class\n",
    "    is_major = (torch.arange(n_classes) <\n",
    "                (1 - minority_frac) * n_classes).float()\n",
    "    major_count = int(\n",
    "        torch.min(is_major * y_counts +\n",
    "                  (1 - is_major) * y_counts * imbalance_ratio).item())\n",
    "    minor_count = int(np.floor(major_count / imbalance_ratio))\n",
    "    print(y_counts, major_count, minor_count)\n",
    "    # subsample\n",
    "    sampled_indices = []\n",
    "    for y in np.arange(n_classes):\n",
    "        (indices, ) = np.where(y_array == y)\n",
    "        np.random.shuffle(indices)\n",
    "        if is_major[y]:\n",
    "            sample_size = major_count\n",
    "        else:\n",
    "            sample_size = minor_count\n",
    "        sampled_indices.append(indices[:sample_size])\n",
    "    sampled_indices = torch.from_numpy(np.concatenate(sampled_indices))\n",
    "    return Subset(dataset, sampled_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfounderDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir,\n",
    "        target_name,\n",
    "        confounder_names,\n",
    "        model_type=None,\n",
    "        augment_data=None,\n",
    "    ):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_group_array(self):\n",
    "        return self.group_array\n",
    "\n",
    "    def get_label_array(self):\n",
    "        return self.y_array\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filename_array)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        y = self.y_array[idx]\n",
    "        g = self.group_array[idx]\n",
    "\n",
    "        if model_attributes[self.model_type][\"feature_type\"] == \"precomputed\":\n",
    "            x = self.features_mat[idx, :]\n",
    "        else:\n",
    "            img_filename = os.path.join(self.data_dir,\n",
    "                                        self.filename_array[idx])\n",
    "            img = Image.open(img_filename).convert(\"RGB\")\n",
    "            # Figure out split and transform accordingly\n",
    "            if self.split_array[idx] == self.split_dict[\n",
    "                    \"train\"] and self.train_transform:\n",
    "                img = self.train_transform(img)\n",
    "            elif (self.split_array[idx]\n",
    "                  in [self.split_dict[\"val\"], self.split_dict[\"test\"]]\n",
    "                  and self.eval_transform):\n",
    "                img = self.eval_transform(img)\n",
    "            # Flatten if needed\n",
    "            if model_attributes[self.model_type][\"flatten\"]:\n",
    "                assert img.dim() == 3\n",
    "                img = img.view(-1)\n",
    "            x = img\n",
    "\n",
    "        return x, y, g, idx\n",
    "\n",
    "    def get_splits(self, splits, train_frac=1.0):\n",
    "        subsets = {}\n",
    "        for split in splits:\n",
    "            assert split in (\"train\", \"val\",\n",
    "                             \"test\"), f\"{split} is not a valid split\"\n",
    "            mask = self.split_array == self.split_dict[split]\n",
    "\n",
    "            num_split = np.sum(mask)\n",
    "            indices = np.where(mask)[0]\n",
    "            if train_frac < 1 and split == \"train\":\n",
    "                num_to_retain = int(np.round(float(len(indices)) * train_frac))\n",
    "                indices = np.sort(\n",
    "                    np.random.permutation(indices)[:num_to_retain])\n",
    "            subsets[split] = Subset(self, indices)\n",
    "        return subsets\n",
    "\n",
    "    def group_str(self, group_idx):\n",
    "        y = group_idx // (self.n_groups / self.n_classes)\n",
    "        c = group_idx % (self.n_groups // self.n_classes)\n",
    "\n",
    "        group_name = f\"{self.target_name} = {int(y)}\"\n",
    "        bin_str = format(int(c), f\"0{self.n_confounders}b\")[::-1]\n",
    "        for attr_idx, attr_name in enumerate(self.confounder_names):\n",
    "            group_name += f\", {attr_name} = {bin_str[attr_idx]}\"\n",
    "        return group_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CUBDataset(ConfounderDataset):\n",
    "    \"\"\"\n",
    "    CUB dataset (already cropped and centered).\n",
    "    NOTE: metadata_df is one-indexed.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir,\n",
    "        target_name,\n",
    "        confounder_names,\n",
    "        augment_data=False,\n",
    "        model_type=None,\n",
    "        data_dir=None,\n",
    "        metadata_csv_name=\"metadata.csv\"\n",
    "    ):\n",
    "        self.root_dir = root_dir\n",
    "        self.target_name = target_name\n",
    "        self.confounder_names = confounder_names\n",
    "        self.model_type = model_type\n",
    "        self.augment_data = augment_data\n",
    "\n",
    "        if data_dir is None:\n",
    "            self.data_dir = os.path.join(\n",
    "                self.root_dir, \"data\",\n",
    "                \"_\".join([self.target_name] + self.confounder_names))\n",
    "        else:\n",
    "            self.data_dir = data_dir\n",
    "\n",
    "        if not os.path.exists(self.data_dir):\n",
    "            raise ValueError(\n",
    "                f\"{self.data_dir} does not exist yet. Please generate the dataset first.\"\n",
    "            )\n",
    "        \n",
    "        print(self.data_dir)\n",
    "\n",
    "        # Read in metadata\n",
    "        print(f\"Reading '{os.path.join(self.data_dir, metadata_csv_name)}'\")\n",
    "        self.metadata_df = pd.read_csv(metadata_csv_name)\n",
    "\n",
    "        # Get the y values\n",
    "        self.y_array = self.metadata_df[\"y\"].values\n",
    "        self.n_classes = 2\n",
    "\n",
    "        # We only support one confounder for CUB for now\n",
    "        self.confounder_array = self.metadata_df[\"place\"].values\n",
    "        self.n_confounders = 1\n",
    "        # Map to groups\n",
    "        self.n_groups = pow(2, 2)\n",
    "        assert self.n_groups == 4, \"check the code if you are running otherwise\"\n",
    "        self.group_array = (self.y_array * (self.n_groups / 2) +\n",
    "                            self.confounder_array).astype(\"int\")\n",
    "\n",
    "        # Extract filenames and splits\n",
    "        self.filename_array = self.metadata_df[\"img_filename\"].values\n",
    "        self.split_array = self.metadata_df[\"split\"].values\n",
    "        self.split_dict = {\n",
    "            \"train\": 0,\n",
    "            \"val\": 1,\n",
    "            \"test\": 2,\n",
    "        }\n",
    "\n",
    "        # Set transform\n",
    "        if model_attributes[self.model_type][\"feature_type\"] == \"precomputed\":\n",
    "            self.features_mat = torch.from_numpy(\n",
    "                np.load(\n",
    "                    os.path.join(\n",
    "                        root_dir,\n",
    "                        \"features\",\n",
    "                        model_attributes[self.model_type][\"feature_filename\"],\n",
    "                    ))).float()\n",
    "            self.train_transform = None\n",
    "            self.eval_transform = None\n",
    "        else:\n",
    "            self.features_mat = None\n",
    "            self.train_transform = get_transform_cub(self.model_type,\n",
    "                                                     train=True,\n",
    "                                                     augment_data=augment_data)\n",
    "            self.eval_transform = get_transform_cub(self.model_type,\n",
    "                                                    train=False,\n",
    "                                                    augment_data=augment_data)\n",
    "\n",
    "\n",
    "def get_transform_cub(model_type, train, augment_data):\n",
    "    scale = 256.0 / 224.0\n",
    "    target_resolution = model_attributes[model_type][\"target_resolution\"]\n",
    "    assert target_resolution is not None\n",
    "\n",
    "    if (not train) or (not augment_data):\n",
    "        # Resizes the image to a slightly larger square then crops the center.\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((\n",
    "                int(target_resolution[0] * scale),\n",
    "                int(target_resolution[1] * scale),\n",
    "            )),\n",
    "            transforms.CenterCrop(target_resolution),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    else:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(\n",
    "                target_resolution,\n",
    "                scale=(0.7, 1.0),\n",
    "                ratio=(0.75, 1.3333333333333333),\n",
    "                interpolation=2,\n",
    "            ),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confounder_settings = {\n",
    "    \"cub\": {\n",
    "        \"constructor\": CUBDataset\n",
    "    },\n",
    "    \"CUB\": {\n",
    "        \"constructor\": CUBDataset\n",
    "    },\n",
    "}\n",
    "\n",
    "def prepare_confounder_data(train, return_full_dataset=False):\n",
    "    full_dataset = confounder_settings[dataset][\"constructor\"](\n",
    "        root_dir=root_dir,\n",
    "        target_name=target,\n",
    "        confounder_names=confounder_name,\n",
    "        model_type=model,\n",
    "        augment_data=augment_data,\n",
    "        data_dir=data_path if (data_path is not None) else None,\n",
    "        metadata_csv_name=metadata_path if (metadata_path is not None) else \"metadata.csv\",\n",
    "    )\n",
    "    if return_full_dataset:\n",
    "        return DRODataset(\n",
    "            full_dataset,\n",
    "            process_item_fn=None,\n",
    "            n_groups=full_dataset.n_groups,\n",
    "            n_classes=full_dataset.n_classes,\n",
    "            group_str_fn=full_dataset.group_str,\n",
    "        )\n",
    "    if train:\n",
    "        splits = [\"train\", \"val\", \"test\"]\n",
    "    else:\n",
    "        splits = [\"test\"]\n",
    "    subsets = full_dataset.get_splits(splits, train_frac=fraction)\n",
    "    dro_subsets = [\n",
    "        DRODataset(\n",
    "            subsets[split],\n",
    "            process_item_fn=None,\n",
    "            n_groups=full_dataset.n_groups,\n",
    "            n_classes=full_dataset.n_classes,\n",
    "            group_str_fn=full_dataset.group_str,\n",
    "        ) for split in splits\n",
    "    ]\n",
    "    return dro_subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_attributes = {\n",
    "    \"cub\": {\n",
    "        \"root_dir\": \"cub\"\n",
    "    },\n",
    "    \"CUB\": {\n",
    "        \"root_dir\": \"cub\"\n",
    "    },\n",
    "}\n",
    "def prepare_data(train, return_full_dataset=False):\n",
    "    global root_dir\n",
    "    # Set root_dir to defaults if necessary\n",
    "    if root_dir is None:\n",
    "        root_dir = dataset_attributes[dataset][\"root_dir\"]\n",
    "    if shift_type == \"confounder\":\n",
    "        return prepare_confounder_data(\n",
    "            train,\n",
    "            return_full_dataset,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confounder_settings = {\n",
    "    \"cub\": {\n",
    "        \"constructor\": CUBDataset\n",
    "    },\n",
    "    \"CUB\": {\n",
    "        \"constructor\": CUBDataset\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spurious_col_csv():\n",
    "    metadata_dir = join(join(results_dir, dataset), exp_name)\n",
    "    # output_dir: results/dataset/exp_name/\n",
    "    output_dir = metadata_dir\n",
    "    # output_path: results/dataset/exp_name/metadata_aug.csv\n",
    "    output_path = join(output_dir, output_csv_name)\n",
    "    new_metadata = pd.read_csv(metadata_path)\n",
    "    split_name = \"split\"\n",
    "\n",
    "    train_data = new_metadata[new_metadata[split_name] == 0]\n",
    "\n",
    "    index_col = \"Unnamed: 0\"\n",
    "    train_data[\"spurious\"] = train_data[\"y\"] != train_data[\"place\"]\n",
    "    index_col = \"img_id\"\n",
    "        \n",
    "    spur_col = train_data[[\"spurious\", index_col]]\n",
    "    new_metadata = pd.merge(\n",
    "        new_metadata, spur_col, how=\"outer\", on=index_col\n",
    "    )\n",
    "    new_metadata = new_metadata.fillna(False)\n",
    "\n",
    "    # Save metadata\n",
    "    new_metadata.to_csv(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"Sets seed\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_loss(yhat, y):\n",
    "    # The torch loss takes in three arguments so we need to split yhat\n",
    "    # It also expects classes in {+1.0, -1.0} whereas by default we give them in {0, 1}\n",
    "    # Furthermore, if y = 1 it expects the first input to be higher instead of the second,\n",
    "    # so we need to swap yhat[:, 0] and yhat[:, 1]...\n",
    "    torch_loss = torch.nn.MarginRankingLoss(margin=1.0, reduction=\"none\")\n",
    "    y = (y.float() * 2.0) - 1.0\n",
    "    return torch_loss(yhat[:, 1], yhat[:, 0], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEOMETRIES = ('cvar', 'chi-square')\n",
    "\n",
    "\n",
    "def cvar_value(p, v, reg):\n",
    "    \"\"\"Returns <p, v> - reg * KL(p, uniform) for Torch tensors\"\"\"\n",
    "    m = p.shape[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        idx = torch.nonzero(p)  # where is annoyingly backwards incompatible\n",
    "        kl = np.log(m) + (p[idx] * torch.log(p[idx])).sum()\n",
    "\n",
    "    return torch.dot(p, v) - reg * kl\n",
    "\n",
    "def chi_square_value(p,v, reg): # TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustLoss(torch.nn.Module):\n",
    "    \"\"\"PyTorch module for the batch robust loss estimator\"\"\"\n",
    "    def __init__(self, size, reg, geometry, tol=1e-4,\n",
    "                 max_iter=1000, debugging=False):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        size : float\n",
    "            Size of the uncertainty set (\\rho for \\chi^2 and \\alpha for CVaR)\n",
    "            Set float('inf') for unconstrained\n",
    "        reg : float\n",
    "            Strength of the regularizer, entropy if geometry == 'cvar'\n",
    "            $\\chi^2$ divergence if geometry == 'chi-square'\n",
    "        geometry : string\n",
    "            Element of GEOMETRIES\n",
    "        tol : float, optional\n",
    "            Tolerance parameter for the bisection\n",
    "        max_iter : int, optional\n",
    "            Number of iterations after which to break the bisection\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.reg = reg\n",
    "        self.geometry = geometry\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        self.debugging = debugging\n",
    "\n",
    "        self.is_erm = size == 0\n",
    "\n",
    "        if geometry not in GEOMETRIES:\n",
    "            raise ValueError('Geometry %s not supported' % geometry)\n",
    "\n",
    "        if geometry == 'cvar' and self.size > 1:\n",
    "            raise ValueError(f'alpha should be < 1 for cvar, is {self.size}')\n",
    "\n",
    "    def best_response(self, v):\n",
    "        size = self.size\n",
    "        reg = self.reg\n",
    "        m = v.shape[0]\n",
    "\n",
    "        if self.geometry == 'cvar':\n",
    "            if self.reg > 0:\n",
    "                if size == 1.0:\n",
    "                    return torch.ones_like(v) / m\n",
    "\n",
    "                def p(eta):\n",
    "                    x = (v - eta) / reg\n",
    "                    return torch.min(torch.exp(x),\n",
    "                                     torch.Tensor([1 / size]).type(x.dtype)) / m\n",
    "\n",
    "                def bisection_target(eta):\n",
    "                    return 1.0 - p(eta).sum()\n",
    "\n",
    "                eta_min = reg * torch.logsumexp(v / reg - np.log(m), 0)\n",
    "                eta_max = v.max()\n",
    "\n",
    "                if torch.abs(bisection_target(eta_min)) <= self.tol:\n",
    "                    return p(eta_min)\n",
    "            else:\n",
    "                cutoff = int(size * m)\n",
    "                surplus = 1.0 - cutoff / (size * m)\n",
    "\n",
    "                p = torch.zeros_like(v)\n",
    "                idx = torch.argsort(v, descending=True)\n",
    "                p[idx[:cutoff]] = 1.0 / (size * m)\n",
    "                if cutoff < m:\n",
    "                    p[idx[cutoff]] = surplus\n",
    "                return p\n",
    "\n",
    "        if self.geometry == 'chi-square':\n",
    "            if (v.max() - v.min()) / v.max() <= MIN_REL_DIFFERENCE:\n",
    "                return torch.ones_like(v) / m\n",
    "\n",
    "            if size == float('inf'):\n",
    "                assert reg > 0\n",
    "\n",
    "                def p(eta):\n",
    "                    return torch.relu(v - eta) / (reg * m)\n",
    "\n",
    "                def bisection_target(eta):\n",
    "                    return 1.0 - p(eta).sum()\n",
    "\n",
    "                eta_min = min(v.sum() - reg * m, v.min())\n",
    "                eta_max = v.max()\n",
    "\n",
    "            else:\n",
    "                assert size < float('inf')\n",
    "\n",
    "                # failsafe for batch sizes small compared to\n",
    "                # uncertainty set size\n",
    "                if m <= 1 + 2 * size:\n",
    "                    out = (v == v.max()).float()\n",
    "                    out /= out.sum()\n",
    "                    return out\n",
    "\n",
    "                if reg == 0:\n",
    "                    def p(eta):\n",
    "                        pp = torch.relu(v - eta)\n",
    "                        return pp / pp.sum()\n",
    "\n",
    "                    def bisection_target(eta):\n",
    "                        pp = p(eta)\n",
    "                        w = m * pp - torch.ones_like(pp)\n",
    "                        return 0.5 * torch.mean(w ** 2) - size\n",
    "\n",
    "                    eta_min = -(1.0 / (np.sqrt(2 * size + 1) - 1)) * v.max()\n",
    "                    eta_max = v.max()\n",
    "                else:\n",
    "                    def p(eta):\n",
    "                        pp = torch.relu(v - eta)\n",
    "\n",
    "                        opt_lam = max(\n",
    "                            reg, torch.norm(pp) / np.sqrt(m * (1 + 2 * size))\n",
    "                        )\n",
    "\n",
    "                        return pp / (m * opt_lam)\n",
    "\n",
    "                    def bisection_target(eta):\n",
    "                        return 1 - p(eta).sum()\n",
    "\n",
    "                    eta_min = v.min() - 1\n",
    "                    eta_max = v.max()\n",
    "\n",
    "        eta_star = bisection(\n",
    "            eta_min, eta_max, bisection_target,\n",
    "            tol=self.tol, max_iter=self.max_iter)\n",
    "\n",
    "        if self.debugging:\n",
    "            return p(eta_star), eta_star\n",
    "        return p(eta_star)\n",
    "\n",
    "    def forward(self, v):\n",
    "        \"\"\"Value of the robust loss\n",
    "        Note that the best response is computed without gradients\n",
    "        Parameters\n",
    "        ----------\n",
    "        v : torch.Tensor\n",
    "            Tensor containing the individual losses on the batch of examples\n",
    "        Returns\n",
    "        -------\n",
    "        loss : torch.float\n",
    "            Value of the robust loss on the batch of examples\n",
    "        \"\"\"\n",
    "        if self.is_erm:\n",
    "            return v.mean()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                p = self.best_response(v)\n",
    "\n",
    "            if self.geometry == 'cvar':\n",
    "                return cvar_value(p, v, self.reg)\n",
    "            elif self.geometry == 'chi-square':\n",
    "                return chi_square_value(p, v, self.reg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossComputer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        criterion,\n",
    "        loss_type,\n",
    "        dataset,\n",
    "        alpha=None,\n",
    "        gamma=0.1,\n",
    "        adj=None,\n",
    "        min_var_weight=0,\n",
    "        step_size=0.01,\n",
    "        normalize_loss=False,\n",
    "        btl=False,\n",
    "        joint_dro_alpha=None,\n",
    "    ):\n",
    "        assert loss_type in [\"group_dro\", \"erm\", \"joint_dro\"]\n",
    "\n",
    "        self.criterion = criterion\n",
    "        self.loss_type = loss_type\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.min_var_weight = min_var_weight\n",
    "        self.step_size = step_size\n",
    "        self.normalize_loss = normalize_loss\n",
    "        self.btl = btl\n",
    "\n",
    "        self.n_groups = dataset.n_groups\n",
    "\n",
    "        self.group_counts = dataset.group_counts().to(device)\n",
    "        self.group_frac = self.group_counts / self.group_counts.sum()\n",
    "        self.group_str = dataset.group_str\n",
    "\n",
    "        if self.loss_type == \"joint_dro\":\n",
    "            # Joint DRO reg should be 0.\n",
    "            assert joint_dro_alpha is not None\n",
    "            self._joint_dro_loss_computer = RobustLoss(\n",
    "                    joint_dro_alpha, 0, \"cvar\")\n",
    "\n",
    "        if adj is not None:\n",
    "            self.adj = torch.from_numpy(adj).float().to(device)\n",
    "        else:\n",
    "            self.adj = torch.zeros(self.n_groups).float().to(device)\n",
    "\n",
    "        if loss_type == \"group_dro\":\n",
    "            assert alpha, \"alpha must be specified\"\n",
    "\n",
    "        # quantities maintained throughout training\n",
    "        self.adv_probs = torch.ones(self.n_groups).to(device) / self.n_groups\n",
    "        self.exp_avg_loss = torch.zeros(self.n_groups).to(device)\n",
    "        self.exp_avg_initialized = torch.zeros(self.n_groups).byte().to(device)\n",
    "\n",
    "        self.reset_stats()\n",
    "\n",
    "    def loss(self, yhat, y, group_idx=None, is_training=False):\n",
    "        # compute per-sample and per-group losses\n",
    "        per_sample_losses = self.criterion(yhat, y)\n",
    "        group_loss, group_count = self.compute_group_avg(\n",
    "            per_sample_losses, group_idx)\n",
    "        group_acc, group_count = self.compute_group_avg(\n",
    "            (torch.argmax(yhat, 1) == y).float(), group_idx)\n",
    "\n",
    "        # update historical losses\n",
    "        self.update_exp_avg_loss(group_loss, group_count)\n",
    "\n",
    "        # compute overall loss\n",
    "        if self.loss_type == \"group_dro\":\n",
    "            if not self.btl:\n",
    "                actual_loss, weights = self.compute_robust_loss(\n",
    "                    group_loss, group_count)\n",
    "            else:\n",
    "                actual_loss, weights = self.compute_robust_loss_btl(\n",
    "                    group_loss, group_count)\n",
    "        elif self.loss_type == \"joint_dro\":\n",
    "            actual_loss = self._joint_dro_loss_computer(per_sample_losses)\n",
    "            weights = None\n",
    "        else:\n",
    "            assert self.loss_type == \"erm\"\n",
    "\n",
    "            actual_loss = per_sample_losses.mean()\n",
    "            weights = None\n",
    "\n",
    "        # update stats\n",
    "        self.update_stats(actual_loss, group_loss, group_acc, group_count,\n",
    "                          weights)\n",
    "\n",
    "        return actual_loss\n",
    "\n",
    "    def compute_robust_loss(self, group_loss, group_count):\n",
    "        adjusted_loss = group_loss\n",
    "        if torch.all(self.adj > 0):\n",
    "            adjusted_loss += self.adj / torch.sqrt(self.group_counts)\n",
    "        if self.normalize_loss:\n",
    "            adjusted_loss = adjusted_loss / (adjusted_loss.sum())\n",
    "        self.adv_probs = self.adv_probs * torch.exp(\n",
    "            self.step_size * adjusted_loss.data)\n",
    "        self.adv_probs = self.adv_probs / (self.adv_probs.sum())\n",
    "\n",
    "        robust_loss = group_loss @ self.adv_probs\n",
    "        return robust_loss, self.adv_probs\n",
    "\n",
    "    def compute_robust_loss_btl(self, group_loss, group_count):\n",
    "        adjusted_loss = self.exp_avg_loss + self.adj / torch.sqrt(\n",
    "            self.group_counts)\n",
    "        return self.compute_robust_loss_greedy(group_loss, adjusted_loss)\n",
    "\n",
    "    def compute_robust_loss_greedy(self, group_loss, ref_loss):\n",
    "        sorted_idx = ref_loss.sort(descending=True)[1]\n",
    "        sorted_loss = group_loss[sorted_idx]\n",
    "        sorted_frac = self.group_frac[sorted_idx]\n",
    "\n",
    "        mask = torch.cumsum(sorted_frac, dim=0) <= self.alpha\n",
    "        weights = mask.float() * sorted_frac / self.alpha\n",
    "        last_idx = mask.sum()\n",
    "        weights[last_idx] = 1 - weights.sum()\n",
    "        weights = sorted_frac * self.min_var_weight + weights * (\n",
    "            1 - self.min_var_weight)\n",
    "\n",
    "        robust_loss = sorted_loss @ weights\n",
    "\n",
    "        # sort the weights back\n",
    "        _, unsort_idx = sorted_idx.sort()\n",
    "        unsorted_weights = weights[unsort_idx]\n",
    "        return robust_loss, unsorted_weights\n",
    "\n",
    "    def compute_group_avg(self, losses, group_idx):\n",
    "        # compute observed counts and mean loss for each group\n",
    "        group_map = (group_idx == torch.arange(\n",
    "            self.n_groups).unsqueeze(1).long().to(device)).float()\n",
    "\n",
    "        group_count = group_map.sum(1)\n",
    "        group_denom = group_count + (group_count == 0).float()  # avoid nans\n",
    "        group_loss = (group_map @ losses.view(-1)) / group_denom\n",
    "        return group_loss, group_count\n",
    "\n",
    "    def update_exp_avg_loss(self, group_loss, group_count):\n",
    "        prev_weights = (1 - self.gamma * (group_count > 0).float()) * (\n",
    "            self.exp_avg_initialized > 0).float()\n",
    "        curr_weights = 1 - prev_weights\n",
    "        self.exp_avg_loss = self.exp_avg_loss * prev_weights + group_loss * curr_weights\n",
    "        self.exp_avg_initialized = (self.exp_avg_initialized >\n",
    "                                    0) + (group_count > 0)\n",
    "\n",
    "    def reset_stats(self):\n",
    "        self.processed_data_counts = torch.zeros(self.n_groups).to(device)\n",
    "        self.update_data_counts = torch.zeros(self.n_groups).to(device)\n",
    "        self.update_batch_counts = torch.zeros(self.n_groups).to(device)\n",
    "        self.avg_group_loss = torch.zeros(self.n_groups).to(device)\n",
    "        self.avg_group_acc = torch.zeros(self.n_groups).to(device)\n",
    "        self.avg_per_sample_loss = 0.0\n",
    "        self.avg_actual_loss = 0.0\n",
    "        self.avg_acc = 0.0\n",
    "        self.batch_count = 0.0\n",
    "\n",
    "    def update_stats(self,\n",
    "                     actual_loss,\n",
    "                     group_loss,\n",
    "                     group_acc,\n",
    "                     group_count,\n",
    "                     weights=None):\n",
    "        # avg group loss\n",
    "        denom = self.processed_data_counts + group_count\n",
    "        denom += (denom == 0).float()\n",
    "        prev_weight = self.processed_data_counts / denom\n",
    "        curr_weight = group_count / denom\n",
    "        self.avg_group_loss = prev_weight * self.avg_group_loss + curr_weight * group_loss\n",
    "\n",
    "        # avg group acc\n",
    "        self.avg_group_acc = prev_weight * self.avg_group_acc + curr_weight * group_acc\n",
    "\n",
    "        # batch-wise average actual loss\n",
    "        denom = self.batch_count + 1\n",
    "        self.avg_actual_loss = (self.batch_count /\n",
    "                                denom) * self.avg_actual_loss + (\n",
    "                                    1 / denom) * actual_loss\n",
    "\n",
    "        # counts\n",
    "        self.processed_data_counts += group_count\n",
    "        if self.loss_type == \"group_dro\":\n",
    "            self.update_data_counts += group_count * ((weights > 0).float())\n",
    "            self.update_batch_counts += ((group_count * weights) > 0).float()\n",
    "        else:\n",
    "            self.update_data_counts += group_count\n",
    "            self.update_batch_counts += (group_count > 0).float()\n",
    "        self.batch_count += 1\n",
    "\n",
    "        # avg per-sample quantities\n",
    "        group_frac = self.processed_data_counts / (\n",
    "            self.processed_data_counts.sum())\n",
    "        self.avg_per_sample_loss = group_frac @ self.avg_group_loss\n",
    "        self.avg_acc = group_frac @ self.avg_group_acc\n",
    "\n",
    "    def get_model_stats(self, model, stats_dict):\n",
    "        model_norm_sq = 0.0\n",
    "        for param in model.parameters():\n",
    "            model_norm_sq += torch.norm(param)**2\n",
    "        stats_dict[\"model_norm_sq\"] = model_norm_sq.item()\n",
    "        stats_dict[\"reg_loss\"] = wd / 2 * model_norm_sq.item()\n",
    "        return stats_dict\n",
    "\n",
    "    def get_stats(self, model=None):\n",
    "        stats_dict = {}\n",
    "        for idx in range(self.n_groups):\n",
    "            stats_dict[f\"avg_loss_group:{idx}\"] = self.avg_group_loss[\n",
    "                idx].item()\n",
    "            stats_dict[f\"exp_avg_loss_group:{idx}\"] = self.exp_avg_loss[\n",
    "                idx].item()\n",
    "            stats_dict[f\"avg_acc_group:{idx}\"] = self.avg_group_acc[idx].item()\n",
    "            stats_dict[\n",
    "                f\"processed_data_count_group:{idx}\"] = self.processed_data_counts[\n",
    "                    idx].item()\n",
    "            stats_dict[\n",
    "                f\"update_data_count_group:{idx}\"] = self.update_data_counts[\n",
    "                    idx].item()\n",
    "            stats_dict[\n",
    "                f\"update_batch_count_group:{idx}\"] = self.update_batch_counts[\n",
    "                    idx].item()\n",
    "\n",
    "        stats_dict[\"avg_actual_loss\"] = self.avg_actual_loss.item()\n",
    "        stats_dict[\"avg_per_sample_loss\"] = self.avg_per_sample_loss.item()\n",
    "        stats_dict[\"avg_acc\"] = self.avg_acc.item()\n",
    "\n",
    "        # Model stats\n",
    "        if model is not None:\n",
    "            stats_dict = self.get_model_stats(model, stats_dict)\n",
    "\n",
    "        return stats_dict\n",
    "\n",
    "    def log_stats(self, logger, is_training):\n",
    "        if logger is None:\n",
    "            return\n",
    "\n",
    "        logger.write(\n",
    "            f\"Average incurred loss: {self.avg_per_sample_loss.item():.3f}  \\n\"\n",
    "        )\n",
    "        logger.write(\n",
    "            f\"Average sample loss: {self.avg_actual_loss.item():.3f}  \\n\")\n",
    "        logger.write(f\"Average acc: {self.avg_acc.item():.3f}  \\n\")\n",
    "        for group_idx in range(self.n_groups):\n",
    "            logger.write(\n",
    "                f\"  {self.group_str(group_idx)}  \"\n",
    "                f\"[n = {int(self.processed_data_counts[group_idx])}]:\\t\"\n",
    "                f\"loss = {self.avg_group_loss[group_idx]:.3f}  \"\n",
    "                f\"exp loss = {self.exp_avg_loss[group_idx]:.3f}  \"\n",
    "                f\"adjusted loss = {self.exp_avg_loss[group_idx] + self.adj[group_idx]/torch.sqrt(self.group_counts)[group_idx]:.3f}  \"\n",
    "                f\"adv prob = {self.adv_probs[group_idx]:3f}   \"\n",
    "                f\"acc = {self.avg_group_acc[group_idx]:.3f}\\n\")\n",
    "        logger.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(\n",
    "    epoch,\n",
    "    model,\n",
    "    optimizer,\n",
    "    loader,\n",
    "    loss_computer,\n",
    "    logger,\n",
    "    csv_logger,\n",
    "    is_training,\n",
    "    show_progress=False,\n",
    "    log_every=50,\n",
    "    scheduler=None,\n",
    "    csv_name=None,\n",
    "    group=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    scheduler is only used inside this function if model is bert.\n",
    "    \"\"\"\n",
    "\n",
    "    if is_training:\n",
    "        model.train()\n",
    "        if (model_type.startswith(\"bert\") and use_bert_params): # or (args.model == \"bert\"):\n",
    "            model.zero_grad()\n",
    "        \n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    if show_progress:\n",
    "        prog_bar_loader = tqdm(loader)\n",
    "    else:\n",
    "        prog_bar_loader = loader\n",
    "\n",
    "    with torch.set_grad_enabled(is_training):\n",
    "\n",
    "        for batch_idx, batch in enumerate(prog_bar_loader):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            x = batch[0]\n",
    "            y = batch[1]\n",
    "            g = batch[2]\n",
    "            data_idx = batch[3]\n",
    "            \n",
    "            if model_type.startswith(\"bert\"):\n",
    "                input_ids = x[:, :, 0]\n",
    "                input_masks = x[:, :, 1]\n",
    "                segment_ids = x[:, :, 2]\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=input_masks,\n",
    "                    token_type_ids=segment_ids,\n",
    "                    labels=y,\n",
    "                )[1]  # [1] returns logits\n",
    "            else:\n",
    "                # outputs.shape: (batch_size, num_classes)\n",
    "                outputs = model(x)\n",
    "\n",
    "                \n",
    "            output_df = pd.DataFrame()\n",
    "\n",
    "            # Calculate stats\n",
    "            if batch_idx == 0:\n",
    "                acc_y_pred = np.argmax(outputs.detach().cpu().numpy(), axis=1)\n",
    "                acc_y_true = y.cpu().numpy()\n",
    "                indices = data_idx.cpu().numpy()\n",
    "                \n",
    "                probs = outputs.detach().cpu().numpy()\n",
    "            else:\n",
    "                acc_y_pred = np.concatenate([\n",
    "                    acc_y_pred,\n",
    "                    np.argmax(outputs.detach().cpu().numpy(), axis=1)\n",
    "                ])\n",
    "                acc_y_true = np.concatenate([acc_y_true, y.cpu().numpy()])\n",
    "                indices = np.concatenate([indices, data_idx.cpu().numpy()])\n",
    "                probs = np.concatenate([probs, outputs.detach().cpu().numpy()], axis = 0)\n",
    "                \n",
    "            assert probs.shape[0] == indices.shape[0]\n",
    "            # TODO: make this cleaner.\n",
    "            run_name = f\"{csv_name}_epoch_{epoch}_val\"\n",
    "            output_df[f\"y_pred_{run_name}\"] = acc_y_pred\n",
    "            output_df[f\"y_true_{run_name}\"] = acc_y_true\n",
    "            output_df[f\"indices_{run_name}\"] = indices\n",
    "            \n",
    "            for class_ind in range(probs.shape[1]):\n",
    "                output_df[f\"pred_prob_{run_name}_{class_ind}\"] = probs[:, class_ind]\n",
    "\n",
    "            loss_main = loss_computer.loss(outputs, y, g, is_training)\n",
    "\n",
    "            if is_training:\n",
    "                if (model_type.startswith(\"bert\") and use_bert_params): \n",
    "                    loss_main.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(),\n",
    "                                                   max_grad_norm)\n",
    "                    scheduler.step()\n",
    "                    optimizer.step()\n",
    "                    model.zero_grad()\n",
    "                else:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss_main.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            if is_training and (batch_idx + 1) % log_every == 0:\n",
    "                run_stats = loss_computer.get_stats(model) \n",
    "                csv_logger.log(epoch, batch_idx, run_stats)\n",
    "\n",
    "                csv_logger.flush()\n",
    "                loss_computer.log_stats(logger, is_training)\n",
    "                loss_computer.reset_stats()\n",
    "\n",
    "        if run_name is not None:\n",
    "            save_dir = \"/\".join(csv_logger.path.split(\"/\")[:-1])\n",
    "            output_df.to_csv(\n",
    "                os.path.join(save_dir, \n",
    "                                f\"output_{group}_epoch_{epoch}.csv\"))\n",
    "            print(\"Saved\", os.path.join(save_dir, \n",
    "                                f\"output_{group}_epoch_{epoch}.csv\"))\n",
    "\n",
    "\n",
    "        if (not is_training) or loss_computer.batch_count > 0:\n",
    "            run_stats = loss_computer.get_stats(model)\n",
    "\n",
    "            csv_logger.log(epoch, batch_idx, run_stats)\n",
    "            csv_logger.flush()\n",
    "            loss_computer.log_stats(logger, is_training)\n",
    "            if is_training:\n",
    "                loss_computer.reset_stats()\n",
    "\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    criterion,\n",
    "    dataset,\n",
    "    logger,\n",
    "    train_csv_logger,\n",
    "    val_csv_logger,\n",
    "    test_csv_logger,\n",
    "    epoch_offset,\n",
    "    csv_name=None,\n",
    "):\n",
    "    model = model.to(device)\n",
    "\n",
    "    # process generalization adjustment stuff\n",
    "    adjustments = [float(c) for c in generalization_adjustment.split(\",\")]\n",
    "    assert len(adjustments) in (1, dataset[\"train_data\"].n_groups)\n",
    "    if len(adjustments) == 1:\n",
    "        adjustments = np.array(adjustments * dataset[\"train_data\"].n_groups)\n",
    "    else:\n",
    "        adjustments = np.array(adjustments)\n",
    "\n",
    "    train_loss_computer = LossComputer(\n",
    "        criterion,\n",
    "        loss_type=loss_type,\n",
    "        dataset=dataset[\"train_data\"],\n",
    "        alpha=alpha,\n",
    "        gamma=gamma,\n",
    "        adj=adjustments,\n",
    "        step_size=robust_step_size,\n",
    "        normalize_loss=use_normalized_loss,\n",
    "        btl=btl,\n",
    "        min_var_weight=minimum_variational_weight,\n",
    "        joint_dro_alpha=joint_dro_alpha,\n",
    "    )\n",
    "\n",
    "    # BERT uses its own scheduler and optimizer\n",
    "    if (model_type.startswith(\"bert\") and use_bert_params): \n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in model.named_parameters()\n",
    "                    if not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\":\n",
    "                wd,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in model.named_parameters()\n",
    "                    if any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\":\n",
    "                0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                          lr=lr,\n",
    "                          eps=adam_epsilon)\n",
    "        t_total = len(dataset[\"train_loader\"]) * n_epochs\n",
    "        print(f\"\\nt_total is {t_total}\\n\")\n",
    "        scheduler = LinearLR(optimizer,\n",
    "                            warmup_steps=warmup_steps,\n",
    "                            t_total=t_total)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(\n",
    "            filter(lambda p: p.requires_grad, model.parameters()),\n",
    "            lr=lr,\n",
    "            momentum=0.9,\n",
    "            weight_decay=wd,\n",
    "        )\n",
    "        if scheduler_flag:\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                \"min\",\n",
    "                factor=0.1,\n",
    "                patience=5,\n",
    "                threshold=0.0001,\n",
    "                min_lr=0,\n",
    "                eps=1e-08,\n",
    "            )\n",
    "        else:\n",
    "            scheduler = None\n",
    "\n",
    "    best_val_acc = 0\n",
    "    for epoch in range(epoch_offset, epoch_offset + n_epochs):\n",
    "        logger.write(\"\\nEpoch [%d]:\\n\" % epoch)\n",
    "        logger.write(f\"Training:\\n\")\n",
    "        run_epoch(\n",
    "            epoch,\n",
    "            model,\n",
    "            optimizer,\n",
    "            dataset[\"train_loader\"],\n",
    "            train_loss_computer,\n",
    "            logger,\n",
    "            train_csv_logger,\n",
    "            is_training=True,\n",
    "            csv_name=csv_name,\n",
    "            show_progress=show_progress,\n",
    "            log_every=log_every,\n",
    "            scheduler=scheduler,\n",
    "            group=\"train\",\n",
    "        )\n",
    "\n",
    "        logger.write(f\"\\nValidation:\\n\")\n",
    "        val_loss_computer =  LossComputer(\n",
    "            criterion,\n",
    "            loss_type=loss_type,\n",
    "            dataset=dataset[\"val_data\"],\n",
    "            alpha=alpha,\n",
    "            gamma=gamma,\n",
    "            adj=adjustments,\n",
    "            step_size=robust_step_size,\n",
    "            normalize_loss=use_normalized_loss,\n",
    "            btl=btl,\n",
    "            min_var_weight=minimum_variational_weight,\n",
    "            joint_dro_alpha=joint_dro_alpha,\n",
    "        )\n",
    "        run_epoch(\n",
    "            epoch,\n",
    "            model,\n",
    "            optimizer,\n",
    "            dataset[\"val_loader\"],\n",
    "            val_loss_computer,\n",
    "            logger,\n",
    "            val_csv_logger,\n",
    "            is_training=False,\n",
    "            csv_name=csv_name,\n",
    "            group=\"val\",\n",
    "        )\n",
    "\n",
    "        # Test set; don't print to avoid peeking\n",
    "        if dataset[\"test_data\"] is not None:\n",
    "            test_loss_computer = LossComputer(\n",
    "                criterion,\n",
    "                loss_type=loss_type,\n",
    "                dataset=dataset[\"test_data\"],\n",
    "                step_size=robust_step_size,\n",
    "                alpha=alpha,\n",
    "                gamma=gamma,\n",
    "                adj=adjustments,\n",
    "                normalize_loss=use_normalized_loss,\n",
    "                btl=btl,\n",
    "                min_var_weight=minimum_variational_weight,\n",
    "                joint_dro_alpha=joint_dro_alpha,\n",
    "            )\n",
    "            run_epoch(\n",
    "                epoch,\n",
    "                model,\n",
    "                optimizer,\n",
    "                dataset[\"test_loader\"],\n",
    "                test_loss_computer,\n",
    "                None,\n",
    "                test_csv_logger,\n",
    "                is_training=False,\n",
    "                csv_name=csv_name,\n",
    "                group=\"test\",\n",
    "            )\n",
    "\n",
    "        # Inspect learning rates\n",
    "        if (epoch + 1) % 1 == 0:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                curr_lr = param_group[\"lr\"]\n",
    "                logger.write(\"Current lr: %f\\n\" % curr_lr)\n",
    "\n",
    "        if scheduler_flag and model_type != \"bert\":\n",
    "            if loss_type == \"group_dro\":\n",
    "                val_loss, _ = val_loss_computer.compute_robust_loss_greedy(\n",
    "                    val_loss_computer.avg_group_loss,\n",
    "                    val_loss_computer.avg_group_loss)\n",
    "            else:\n",
    "                val_loss = val_loss_computer.avg_actual_loss\n",
    "            scheduler.step(\n",
    "                val_loss)  # scheduler step to update lr at the end of epoch\n",
    "\n",
    "        if epoch % save_step == 0:\n",
    "            torch.save(model, os.path.join(log_dir,\n",
    "                                           \"%d_model.pth\" % epoch))\n",
    "\n",
    "        if save_last:\n",
    "            torch.save(model, os.path.join(log_dir, \"last_model.pth\"))\n",
    "\n",
    "        if save_best:\n",
    "            if loss_type == \"group_dro\" or reweight_groups:\n",
    "                curr_val_acc = min(val_loss_computer.avg_group_acc)\n",
    "            else:\n",
    "                curr_val_acc = val_loss_computer.avg_acc\n",
    "            logger.write(f\"Current validation accuracy: {curr_val_acc}\\n\")\n",
    "            if curr_val_acc > best_val_acc:\n",
    "                best_val_acc = curr_val_acc\n",
    "                torch.save(model, os.path.join(log_dir, \"best_model.pth\"))\n",
    "                logger.write(f\"Best model saved at epoch {epoch}\\n\")\n",
    "\n",
    "        if automatic_adjustment:\n",
    "            gen_gap = val_loss_computer.avg_group_loss - train_loss_computer.exp_avg_loss\n",
    "            adjustments = gen_gap * torch.sqrt(\n",
    "                train_loss_computer.group_counts)\n",
    "            train_loss_computer.adj = adjustments\n",
    "            logger.write(\"Adjustments updated\\n\")\n",
    "            for group_idx in range(train_loss_computer.n_groups):\n",
    "                logger.write(\n",
    "                    f\"  {train_loss_computer.get_group_name(group_idx)}:\\t\"\n",
    "                    f\"adj = {train_loss_computer.adj[group_idx]:.3f}\\n\")\n",
    "        logger.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger(os.path.join(log_dir, \"log.txt\"), mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "# Test data for label_shift_step is not implemented yet\n",
    "test_data = None\n",
    "test_loader = None\n",
    "if shift_type == \"confounder\":\n",
    "    train_data, val_data, test_data = prepare_data(\n",
    "        train=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "###################### Prepare data for our method ######################\n",
    "#########################################################################\n",
    "\n",
    "# Should probably not be upweighting if folds are specified.\n",
    "assert not fold or not up_weight\n",
    "\n",
    "# Fold passed. Use it as train and valid.\n",
    "if fold:\n",
    "    train_data, val_data = get_fold(\n",
    "        train_data,\n",
    "        fold,\n",
    "        cross_validation_ratio=(1 / num_folds_per_sweep),\n",
    "        num_valid_per_point=num_sweeps,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "if up_weight != 0:\n",
    "    assert aug_col is not None\n",
    "    # Get points that should be upsampled\n",
    "    metadata_df = pd.read_csv(metadata_path)\n",
    "    train_col = metadata_df[metadata_df[\"split\"] == 0]\n",
    "    aug_indices = np.where(train_col[aug_col] == 1)[0]\n",
    "    print(\"len\", len(train_col), len(aug_indices))\n",
    "    if up_weight == -1:\n",
    "        up_weight_factor = int(\n",
    "            (len(train_col) - len(aug_indices)) / len(aug_indices)) - 1\n",
    "    else:\n",
    "        up_weight_factor = up_weight\n",
    "\n",
    "    print(f\"Up-weight factor: {up_weight_factor}\")\n",
    "    upsampled_points = Subset(train_data,\n",
    "                                list(aug_indices) * up_weight_factor)\n",
    "    # Convert to DRODataset\n",
    "    train_data = DRODataset(\n",
    "        ConcatDataset([train_data, upsampled_points]),\n",
    "        process_item_fn=None,\n",
    "        n_groups=train_data.n_groups,\n",
    "        n_classes=train_data.n_classes,\n",
    "        group_str_fn=train_data.group_str,\n",
    "    )\n",
    "elif aug_col is not None:\n",
    "    print(\"\\n\"*2 + \"WARNING: aug_col is not being used.\" + \"\\n\"*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "#########################################################################\n",
    "#########################################################################\n",
    "\n",
    "loader_kwargs = {\n",
    "    \"batch_size\": batch_size,\n",
    "    \"num_workers\": 4,\n",
    "    \"pin_memory\": True,\n",
    "}\n",
    "train_loader = get_loader(train_data,\n",
    "                        train=True,\n",
    "                        reweight_groups=reweight_groups,\n",
    "                        **loader_kwargs)\n",
    "\n",
    "val_loader = get_loader(val_data,\n",
    "                        train=False,\n",
    "                        reweight_groups=None,\n",
    "                        **loader_kwargs)\n",
    "\n",
    "if test_data is not None:\n",
    "    test_loader = get_loader(test_data,\n",
    "                            train=False,\n",
    "                            reweight_groups=None,\n",
    "                            **loader_kwargs)\n",
    "\n",
    "data = {}\n",
    "data[\"train_loader\"] = train_loader\n",
    "data[\"val_loader\"] = val_loader\n",
    "data[\"test_loader\"] = test_loader\n",
    "data[\"train_data\"] = train_data\n",
    "data[\"val_data\"] = val_data\n",
    "data[\"test_data\"] = test_data\n",
    "\n",
    "n_classes = train_data.n_classes\n",
    "\n",
    "log_data(data, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize model\n",
    "model = get_model(\n",
    "    model=model,\n",
    "    pretrained=not train_from_scratch,\n",
    "    resume=resume,\n",
    "    n_classes=train_data.n_classes,\n",
    "    dataset=dataset,\n",
    "    log_dir=log_dir,\n",
    ")\n",
    "\n",
    "logger.flush()\n",
    "\n",
    "## Define the objective\n",
    "if hinge:\n",
    "    assert dataset in [\"CUB\"]  # Only supports binary\n",
    "    criterion = hinge_loss\n",
    "else:\n",
    "    criterion = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "if resume:\n",
    "    raise NotImplementedError  # Check this implementation.\n",
    "    df = pd.read_csv(os.path.join(args.log_dir, \"test.csv\"))\n",
    "    epoch_offset = df.loc[len(df) - 1, \"epoch\"] + 1\n",
    "    logger.write(f\"starting from epoch {epoch_offset}\")\n",
    "else:\n",
    "    epoch_offset = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_logger = CSVBatchLogger(os.path.join(log_dir, f\"train.csv\"),\n",
    "                                    train_data.n_groups,\n",
    "                                    mode=mode)\n",
    "val_csv_logger = CSVBatchLogger(os.path.join(log_dir, f\"val.csv\"),\n",
    "                                val_data.n_groups,\n",
    "                                mode=mode)\n",
    "test_csv_logger = CSVBatchLogger(os.path.join(log_dir, f\"test.csv\"),\n",
    "                                    test_data.n_groups,\n",
    "                                    mode=mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    model,\n",
    "    criterion,\n",
    "    data,\n",
    "    logger,\n",
    "    train_csv_logger,\n",
    "    val_csv_logger,\n",
    "    test_csv_logger,\n",
    "    epoch_offset=epoch_offset,\n",
    "    csv_name=fold\n",
    ")\n",
    "\n",
    "train_csv_logger.close()\n",
    "val_csv_logger.close()\n",
    "test_csv_logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_epoch = n_epochs - 1\n",
    "folder_name = \"ERM_upweight_0_epochs_3_lr_1e-05_weight_decay_1.0\"\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(log_dir, f\"output_train_epoch_{final_epoch}.csv\"))\n",
    "train_df = train_df.sort_values(f\"indices_None_epoch_{final_epoch}_val\")\n",
    "train_df[\"wrong_1_times\"] = (1.0 * (train_df[f\"y_pred_None_epoch_{final_epoch}_val\"] != train_df[f\"y_true_None_epoch_{final_epoch}_val\"])).apply(np.int64)\n",
    "print(\"Total wrong\", np.sum(train_df['wrong_1_times']), \"Total points\", len(train_df))\n",
    "\n",
    "original_df = pd.read_csv(metadata_path)\n",
    "original_train_df = original_df[original_df[\"split\"] == 0]\n",
    "merged_csv = original_train_df.join(train_df.set_index(f\"indices_None_epoch_{final_epoch}_val\"))\n",
    "merged_csv[\"spurious\"] = merged_csv['y'] != merged_csv[\"place\"]\n",
    "\n",
    "merged_csv[\"our_spurious\"] = merged_csv[\"spurious\"] & merged_csv[\"wrong_1_times\"]\n",
    "merged_csv[\"our_nonspurious\"] = (merged_csv[\"spurious\"] == 0) & merged_csv[\"wrong_1_times\"]\n",
    "print(\"Number of our spurious: \", np.sum(merged_csv[\"our_spurious\"]))\n",
    "print(\"Number of our nonspurious:\", np.sum(merged_csv[\"our_nonspurious\"]))\n",
    "\n",
    "train_probs_df= merged_csv.fillna(0)\n",
    "\n",
    "spur_precision = np.sum(\n",
    "        (merged_csv[f\"wrong_1_times\"] == 1) & (merged_csv[\"spurious\"] == 1)\n",
    "    ) / np.sum((merged_csv[f\"wrong_1_times\"] == 1))\n",
    "print(\"Spurious precision\", spur_precision)\n",
    "spur_recall = np.sum(\n",
    "    (merged_csv[f\"wrong_1_times\"] == 1) & (merged_csv[\"spurious\"] == 1)\n",
    "    ) / np.sum((merged_csv[\"spurious\"] == 1))\n",
    "print(\"Spurious recall\", spur_recall)\n",
    "\n",
    "probs = softmax(np.array(train_probs_df[[f\"pred_prob_None_epoch_{final_epoch}_val_0\", f\"pred_prob_None_epoch_{final_epoch}_val_1\"]]), axis = 1)\n",
    "train_probs_df[\"probs_0\"] = probs[:,0]\n",
    "train_probs_df[\"probs_1\"] = probs[:,1]\n",
    "train_probs_df[\"confidence\"] = train_probs_df[\"y\"] * train_probs_df[\"probs_1\"] + (1 - train_probs_df[\"y\"]) * train_probs_df[\"probs_0\"]\n",
    "train_probs_df[f\"confidence_thres{conf_threshold}\"] = (train_probs_df[\"confidence\"] < conf_threshold).apply(np.int64)\n",
    "\n",
    "if not os.path.exists(f\"results/{dataset}/{exp_name}/train_downstream_{folder_name}/final_epoch{final_epoch}\"):\n",
    "    os.makedirs(f\"results/{dataset}/{exp_name}/train_downstream_{folder_name}/final_epoch{final_epoch}\")\n",
    "root = f\"results/{dataset}/{exp_name}/train_downstream_{folder_name}/final_epoch{final_epoch}\"\n",
    "\n",
    "train_probs_df.to_csv(f\"{root}/metadata_aug.csv\")\n",
    "root = f\"{exp_name}/train_downstream_{folder_name}/final_epoch{final_epoch}\"\n",
    "\n",
    "sbatch_command = (\n",
    "        f\"python generate_downstream.py --exp_name {root} --lr {lr} --weight_decay {wd} --method JTT --dataset {dataset} --aug_col {aug_col}\" + (f\" --batch_size {batch_size}\" if batch_size else \"\")\n",
    "    )\n",
    "print(sbatch_command)\n",
    "if deploy:\n",
    "    subprocess.run(sbatch_command, check=True, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_df(df):\n",
    "    \"\"\"\n",
    "    Fix a results df for problems arising from resuming.\n",
    "    \"\"\"\n",
    "    # Remove stray epoch/batches\n",
    "    duplicates = df.duplicated(subset=[\"epoch\", \"batch\"], keep=\"last\")\n",
    "    df = df.loc[~duplicates, :]\n",
    "    df.index = np.arange(len(df))\n",
    "\n",
    "    if np.sum(duplicates) > 0:\n",
    "        print(\n",
    "            f\"Removed {np.sum(duplicates)} duplicates from epochs {np.unique(df.loc[duplicates, 'epoch'])}\"\n",
    "        )\n",
    "\n",
    "    # Make sure epoch/batch is increasing monotonically\n",
    "    prev_epoch = -1\n",
    "    prev_batch = -1\n",
    "    last_batch_in_epoch = -1\n",
    "    for i in range(len(df)):\n",
    "        try:\n",
    "            epoch, batch = df.loc[i, [\"epoch\", \"batch\"]].astype(int)\n",
    "        except:\n",
    "            print(i, epoch, batch, len(df))\n",
    "        assert ((prev_epoch == epoch) and\n",
    "                (prev_batch < batch)) or ((prev_epoch == epoch - 1))\n",
    "        if prev_epoch == epoch - 1:\n",
    "            assert (last_batch_in_epoch == -1) or (last_batch_in_epoch\n",
    "                                                   == prev_batch)\n",
    "            last_batch_in_epoch = prev_batch\n",
    "        prev_epoch = epoch\n",
    "        prev_batch = batch\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(train_df, val_df, test_df, n_groups):\n",
    "    loss_metrics = []\n",
    "    acc_metrics = []\n",
    "    for group_idx in range(n_groups):  # 4 groups\n",
    "        loss_metrics.append(f\"avg_loss_group:{group_idx}\")\n",
    "        acc_metrics.append(f\"avg_acc_group:{group_idx}\")\n",
    "    # robust acc\n",
    "    for df in [train_df, val_df, test_df]:\n",
    "        try:\n",
    "            df[\"robust_loss\"] = np.max(df.loc[:, loss_metrics], axis=1)\n",
    "            df[\"robust_acc\"] = np.min(df.loc[:, acc_metrics], axis=1)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df_waterbird9(train_df, val_df, test_df, params):\n",
    "    process_df(train_df, val_df, test_df, params)\n",
    "    loss_metrics = []\n",
    "    acc_metrics = []\n",
    "    for group_idx in range(params[\"n_groups\"]):\n",
    "        loss_metrics.append(f\"avg_loss_group:{group_idx}\")\n",
    "        acc_metrics.append(f\"avg_acc_group:{group_idx}\")\n",
    "\n",
    "    ratio = params[\"n_train\"] / np.sum(params[\"n_train\"])\n",
    "    val_df[\"avg_acc\"] = val_df.loc[:, acc_metrics] @ ratio\n",
    "    val_df[\"avg_loss\"] = val_df.loc[:, loss_metrics] @ ratio\n",
    "    test_df[\"avg_acc\"] = test_df.loc[:, acc_metrics] @ ratio\n",
    "    test_df[\"avg_loss\"] = test_df.loc[:, loss_metrics] @ ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accs_for_epoch_across_batches(df, epoch):\n",
    "    n_groups = 1 + np.max([\n",
    "        int(col.split(\":\")[1])\n",
    "        for col in df.columns if col.startswith(\"avg_acc_group\")\n",
    "    ])\n",
    "\n",
    "    indices = df[\"epoch\"] == epoch\n",
    "\n",
    "    accs = np.zeros(n_groups)\n",
    "    total_counts = np.zeros(n_groups)\n",
    "    correct_counts = np.zeros(n_groups)\n",
    "\n",
    "    for i in np.where(indices)[0]:\n",
    "        for group in range(n_groups):\n",
    "            total_counts[group] += df.loc[\n",
    "                i, f\"processed_data_count_group:{group}\"]\n",
    "            correct_counts[group] += np.round(\n",
    "                df.loc[i, f\"avg_acc_group:{group}\"] *\n",
    "                df.loc[i, f\"processed_data_count_group:{group}\"])\n",
    "\n",
    "    accs = correct_counts / total_counts\n",
    "    robust_acc = np.min(accs)\n",
    "    avg_acc = accs @ total_counts / np.sum(total_counts)\n",
    "    return avg_acc, robust_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_accs(\n",
    "    dfs,\n",
    "    output_dir,\n",
    "    params=None,\n",
    "    epoch_to_eval=None,\n",
    "    print_avg=False,\n",
    "    output=True,\n",
    "    splits=[\"train\", \"val\", \"test\"],\n",
    "    early_stop=True,\n",
    "    print_groups = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Input: dictionary of dfs with keys 'val', 'test'\n",
    "    This takes the minority group 'n' for calculating stdev,\n",
    "    which is conservative.\n",
    "    Since clean val/test acc for waterbirds is estimated from a val/test set with a different distribution, there's probably a bit more variability,\n",
    "    but this is minor since the overall n is high.\n",
    "    \"\"\"\n",
    "    for split in splits:\n",
    "        assert split in dfs\n",
    "\n",
    "    early_stopping_epoch = np.argmax(dfs[\"val\"][\"robust_acc\"].values)\n",
    "\n",
    "    epochs = []\n",
    "    assert early_stop or (epoch_to_eval is not None)\n",
    "    if early_stop:\n",
    "        epochs += [(\"early stop at epoch\", \"early_stopping\",\n",
    "                    early_stopping_epoch)]\n",
    "    if epoch_to_eval is not None:\n",
    "        epochs += [(\"epoch\", \"epoch_to_eval\", epoch_to_eval)]\n",
    "\n",
    "    metrics = [(\"Val Robust Worst Group\", \"robust_acc\")]\n",
    "    if print_avg:\n",
    "        metrics += [(\"Val Average Acc\", \"avg_acc\")]\n",
    "    if print_groups: \n",
    "        for i in range(group_count): #  group_count = np.max(np.array([col.split(\":\")[1] for col in val_df.columns if \"_group\" in col]).astype(int)) + 1\n",
    "            metrics += [(f\"group {i} acc\", f\"avg_acc_group:{i}\")]\n",
    "\n",
    "    results = {}\n",
    "    for metric_str, metric in metrics:\n",
    "        results[metric] = {}\n",
    "\n",
    "        for split in splits:\n",
    "            for epoch_print_str, epoch_save_str, epoch in epochs:\n",
    "                if epoch not in dfs[split][\"epoch\"].values:\n",
    "                    if output:\n",
    "                        print(\n",
    "                            f\"{metric_str} {split:<5} acc ({epoch_print_str} {epoch_to_eval}):               Not yet run\"\n",
    "                        )\n",
    "                else:\n",
    "                    if split == \"train\":\n",
    "                        avg_acc, robust_acc = get_accs_for_epoch_across_batches(\n",
    "                            dfs[split], epoch)\n",
    "                        if metric == \"avg_acc\":\n",
    "                            acc = avg_acc\n",
    "                        elif metric == \"robust_acc\":\n",
    "                            acc = robust_acc\n",
    "                    else:\n",
    "                        idx = np.where(dfs[split][\"epoch\"] == epoch)[0][\n",
    "                            -1]  # Take the last batch in this epoch\n",
    "                        acc = dfs[split].loc[idx, metric]\n",
    "\n",
    "                    if split not in results[metric]:\n",
    "                        results[metric][split] = {}\n",
    "\n",
    "                    if params is None:\n",
    "                        if output:\n",
    "                            print(\n",
    "                                f\"{metric_str} {split:<5} acc ({epoch_print_str} {epoch}): \"\n",
    "                                f\"{acc*100:.1f}\")\n",
    "                            with open(output_dir + \"/val_accuracies.txt\",\n",
    "                                      \"a\") as text_file:\n",
    "                                print(\n",
    "                                    f\"{metric_str} {split:<5} acc ({epoch_print_str} {epoch}): \"\n",
    "                                    f\"{acc*100:.1f}\",\n",
    "                                    file=text_file,\n",
    "                                )\n",
    "                    else:\n",
    "                        n_str = f\"n_{split}\"\n",
    "                        minority_n = np.min(params[n_str])\n",
    "                        total_n = np.sum(params[n_str])\n",
    "                        if metric == \"robust_acc\":\n",
    "                            n = minority_n\n",
    "                        elif metric == \"avg_acc\":\n",
    "                            n = total_n\n",
    "\n",
    "                        stddev = np.sqrt(acc * (1 - acc) / n)\n",
    "                        results[metric][split][epoch_save_str] = (acc, stddev)\n",
    "\n",
    "                        if output:\n",
    "                            print(\n",
    "                                f\"{metric_str} {split:<5} acc ({epoch_print_str} {epoch}): \"\n",
    "                                f\"{acc*100:.1f} ({stddev*100:.1f})\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"CUB/CUB_sample_exp\"\n",
    "runs = [\"ERM_upweight_0_epochs_3_lr_1e-05_weight_decay_1.0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print robust val accuracies from downstream runs\n",
    "for run in runs:\n",
    "    try:\n",
    "        sub_exp_name = run\n",
    "        \n",
    "        training_output_dir = os.path.join(output_dir,\n",
    "                                        sub_exp_name, \"model_outputs\")\n",
    "        train_path = os.path.join(training_output_dir, \"train.csv\")\n",
    "        val_path = os.path.join(training_output_dir, \"val.csv\")\n",
    "        test_path = os.path.join(training_output_dir, \"test.csv\")\n",
    "        train_df = pd.read_csv(train_path)\n",
    "        val_df = pd.read_csv(val_path)\n",
    "        test_df = pd.read_csv(test_path)\n",
    "        group_count = np.max(np.array([col.split(\":\")[1] for col in val_df.columns if \"_group\" in col]).astype(int)) + 1\n",
    "        \n",
    "        process_df(train_df, val_df, test_df, n_groups=group_count)\n",
    "\n",
    "        dfs = {}\n",
    "        dfs[\"train\"] = train_df\n",
    "        dfs[\"val\"] = val_df\n",
    "        dfs[\"test\"] = test_df\n",
    "        \n",
    "        print(f\"Downstream Accuracies for {sub_exp_name} with {group_count} groups.\")\n",
    "        with open(training_output_dir + \"/val_accuracies.txt\", \"a\") as text_file:\n",
    "            print(f\"Downstream Accuracies for {sub_exp_name}\", file=text_file)\n",
    "            \n",
    "        # Print average and worst group accuracies for val\n",
    "        print_accs(\n",
    "            dfs,\n",
    "            training_output_dir,\n",
    "            params=None,\n",
    "            epoch_to_eval=None,\n",
    "            print_avg=True,\n",
    "            print_groups=True,\n",
    "            output=True,\n",
    "            splits=[\"val\", 'test'],\n",
    "            early_stop=True,\n",
    "        )\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    except:\n",
    "        import sys\n",
    "        if str(sys.exc_info()[0]) != \"<class 'FileNotFoundError'>\":\n",
    "            print(\"\\n\")\n",
    "            print(f\"problem with {run}\")\n",
    "            print(sys.exc_info())\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
