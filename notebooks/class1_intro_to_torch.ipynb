{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello, this is introductory notebook based on https://pytorch.org/tutorials/beginner/basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors\n",
    "\n",
    "Tensors are a specialized data structure that are very similar to arrays and matrices.\n",
    "In PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model’s parameters.\n",
    "\n",
    "Tensors are similar to [NumPy’s](https://numpy.org/) ndarrays, except that tensors can run on GPUs or other hardware accelerators. In fact, tensors and\n",
    "NumPy arrays can often share the same underlying memory, eliminating the need to copy data (see `bridge-to-np-label`). Tensors\n",
    "are also optimized for automatic differentiation (we'll see more about that later in the [Autograd](autogradqs_tutorial.html)_\n",
    "section). If you’re familiar with ndarrays, you’ll be right at home with the Tensor API. If not, follow along!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 2],[3, 4]]\n",
    "x_data = torch.tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, most of the operations on numpy arrays work the same way for torch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attributes of a Tensor\n",
    "\n",
    "Tensor attributes describe their shape, datatype, and the device on which they are stored.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Joining tensors** You can use ``torch.cat`` to concatenate a sequence of tensors along a given dimension.\n",
    "See also [torch.stack](https://pytorch.org/docs/stable/generated/torch.stack.html)_,\n",
    "another tensor joining operator that is subtly different from ``torch.cat``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arithmetics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\n",
    "# ``tensor.T`` returns the transpose of a tensor\n",
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "\n",
    "y3 = torch.rand_like(y1)\n",
    "torch.matmul(tensor, tensor.T, out=y3)\n",
    "\n",
    "\n",
    "# This computes the element-wise product. z1, z2, z3 will have the same value\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{tensor} \\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We move our tensor to the GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors to numpy / scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = tensor.sum()\n",
    "agg_item = agg.item()\n",
    "print(agg_item, type(agg_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We skip standard ML models and jump straigth into NNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 28\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(IMG_SIZE*IMG_SIZE, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_path = \"../data\"\n",
    "os.makedirs(data_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=data_path,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=data_path,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function (a.k.a. objective function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross entropy: $$ \\mathcal{L}_{CE} = - \\sum_{i}^{C} y_i log [ f(x_i) ] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3 # 0.001\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proceedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving / Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "\n",
    "path_to_model = \"../models/model.pth\"\n",
    "\n",
    "torch.save(model.state_dict(), path_to_model)\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "another_model = NeuralNetwork().to(device)\n",
    "another_model.load_state_dict(torch.load(path_to_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    x = x.to(device)\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VARIATIONAL AUTOENCODER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More complex model"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib.offsetbox import AnnotationBbox, OffsetImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.fc_1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_mean  = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_var   = nn.Linear (hidden_dim, latent_dim)\n",
    "        \n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        self.training = True\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x       = self.LeakyReLU(self.fc_1(x))\n",
    "        x       = self.LeakyReLU(self.fc_2(x))\n",
    "        mean     = self.fc_mean(x)\n",
    "        log_var  = self.fc_var(x)\n",
    "        \n",
    "        return mean, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc_1 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc_2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_3 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h     = self.LeakyReLU(self.fc_1(x))\n",
    "        h     = self.LeakyReLU(self.fc_2(h))\n",
    "        \n",
    "        x_hat = torch.sigmoid(self.fc_3(h))\n",
    "        x_hat = x_hat.view([-1, 1, 28, 28])\n",
    "        return x_hat\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, x_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = Encoder(input_dim=x_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)\n",
    "        self.decoder = Decoder(latent_dim=latent_dim, hidden_dim = hidden_dim, output_dim = x_dim)\n",
    "\n",
    "        \n",
    "    def reparameterization(self, mean, var):\n",
    "        epsilon = torch.randn_like(var).to(device)        # sampling epsilon        \n",
    "        z = mean + var * epsilon                          # reparameterization trick\n",
    "#         z = mean # Change to proper sampling\n",
    "        return z\n",
    "        \n",
    "                \n",
    "    def forward(self, x):\n",
    "        mean, log_var = self.encoder(x)\n",
    "        z = self.reparameterization(mean, torch.exp(0.5 * log_var)) # takes exponential function (log var -> var)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat, mean, log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset = FashionMNIST(root=\"../data/\", train=True, transform=transform, download=True)\n",
    "test_set = FashionMNIST(root=\"../data/\", train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=256, shuffle=True, drop_last=True, pin_memory=True, num_workers=4)\n",
    "test_loader = data.DataLoader(test_set, batch_size=256, shuffle=False, drop_last=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(latent_dim=32, hidden_dim=256, x_dim=784).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss_function(x, x_hat, mean, log_var):\n",
    "    reproduction_loss = nn.functional.mse_loss(x_hat, x, reduction='sum')\n",
    "    KLD      = -0.5 * torch.sum(1+ log_var - mean.pow(2) - log_var.exp())\n",
    "\n",
    "    return reproduction_loss + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion1 = nn.MSELoss(reduction=\"sum\")\n",
    "criterion2 = vae_loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = criterion1\n",
    "optimizer = optim.Adam(vae.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "for n in range(num_epochs):\n",
    "    losses_epoch = []\n",
    "    for x, _ in iter(train_loader):\n",
    "        x = x.to(device)\n",
    "        out, means, log_var = vae(x)\n",
    "        loss = criterion(out, x) \n",
    "        losses_epoch.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()  \n",
    "    L1_list = []\n",
    "    for x, _ in iter(test_loader):\n",
    "        x  = x.to(device)\n",
    "        out, _, _ = vae(x)\n",
    "        L1_list.append(torch.mean(torch.abs(out-x)).item())\n",
    "    print(f\"Epoch {n} loss {np.mean(np.array(losses_epoch))}, test L1 = {np.mean(L1_list)}\")\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_reconstructions(model, input_imgs, device):\n",
    "    # Reconstruct images\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        reconst_imgs, means, log_var = model(input_imgs.to(device))\n",
    "    reconst_imgs = reconst_imgs.cpu()\n",
    "    \n",
    "    # Plotting\n",
    "    imgs = torch.stack([input_imgs, reconst_imgs], dim=1).flatten(0,1)\n",
    "    grid = torchvision.utils.make_grid(imgs, nrow=4, normalize=False, range=(-1,1))\n",
    "    grid = grid.permute(1, 2, 0)\n",
    "    if len(input_imgs) == 4:\n",
    "        plt.figure(figsize=(10,10))\n",
    "    else:\n",
    "        plt.figure(figsize=(15,10))\n",
    "    plt.title(f\"Reconstructions\")\n",
    "    plt.imshow(grid)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_images(num):\n",
    "    return torch.stack([test_set[i][0] for i in range(10,10+num)], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_imgs = get_train_images(8)\n",
    "visualize_reconstructions(vae, input_imgs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(model, n_imgs, device):\n",
    "    # Reconstruct images\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated_imgs = model.decoder(torch.randn([n_imgs, model.latent_dim]).to(device))\n",
    "    generated_imgs = generated_imgs.cpu()\n",
    "    \n",
    "    # Plotting\n",
    "    # imgs = generated_imgs.flatten(0,1)\n",
    "    grid = torchvision.utils.make_grid(generated_imgs, nrow=4, normalize=False, range=(-1,1))\n",
    "    grid = grid.permute(1, 2, 0)\n",
    "    if len(input_imgs) == 4:\n",
    "        plt.figure(figsize=(10,10))\n",
    "    else:\n",
    "        plt.figure(figsize=(15,10))\n",
    "    plt.title(f\"Generations\")\n",
    "    plt.imshow(grid)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_images(vae, 16 , device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_imgs(model, data_loader):\n",
    "    # Encode all images in the data_laoder using model, and return both images and encodings\n",
    "    img_list, embed_list = [], []\n",
    "    model.eval()\n",
    "    labels = []\n",
    "    for imgs, label in data_loader:\n",
    "        with torch.no_grad():\n",
    "            mean, var_log = model.encoder(imgs.to(device))\n",
    "        img_list.append(imgs)\n",
    "        embed_list.append(mean)\n",
    "        labels.append(label)\n",
    "    return (torch.cat(img_list, dim=0), torch.cat(embed_list, dim=0), torch.cat(labels, dim=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_object = umap.UMAP(metric=\"cosine\", n_neighbors=100)\n",
    "train_img_embeds = embed_imgs(vae, train_loader)\n",
    "test_img_embeds = embed_imgs(vae, test_loader)\n",
    "train_embedded = umap_object.fit_transform(train_img_embeds[1][:5000].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent(train_embedded, train_img_embeds, n_data=5000):\n",
    "    data = pd.DataFrame(train_embedded[:n_data])\n",
    "    data[\"label\"] = train_img_embeds[2][:n_data].cpu().numpy()\n",
    "    examples = []\n",
    "    examples_locations = []\n",
    "    for i in np.random.randint(0,n_data,40):\n",
    "        examples.append(train_img_embeds[0][i].squeeze(0).cpu().numpy())\n",
    "        examples_locations.append(data.iloc[i])\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    # ax.scatter(noises_to_plot_tsne[0],noises_to_plot_tsne[1],c=noises_to_plot_tsne[\"batch\"],s=3,alpha=0.8)\n",
    "    sns.scatterplot(\n",
    "        x=0, y=1,\n",
    "        hue=\"label\",\n",
    "        palette=sns.color_palette(\"hls\", 10),\n",
    "        data=data,\n",
    "        legend=\"full\",\n",
    "        alpha=0.1\n",
    "    )\n",
    "    for location, example in zip(examples_locations, examples):\n",
    "        x, y = location[0], location[1]\n",
    "        label = int(location[\"label\"])\n",
    "        ab = AnnotationBbox(OffsetImage(example,cmap=plt.cm.gray_r, zoom=1), (x, y), frameon=True,\n",
    "                            bboxprops=dict(facecolor=sns.color_palette(\"hls\", 10)[label], boxstyle=\"round\"))\n",
    "        ax.add_artist(ab)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latent(train_embedded, train_img_embeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
